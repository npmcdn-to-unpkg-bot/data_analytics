{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"NB\"> Naive Bayes Classification </h2>\n",
    "\n",
    "Given the feature vector $X=[x_1,x_2,\\ldots,x_n]$, we want to find the class y among the set of classes Y. The naive bayes classifiers proceeds starting from the Bayes' rule as the name suggests\n",
    "\n",
    "\\begin{align}\n",
    "p(y|X)&=\\frac{p(X,y)}{p(y)}=\\frac{p(x_1,x_2,\\ldots,x_n,y)}{p(y)}\\\\\n",
    "&=p(x_1|x_2,\\ldots,x_n,y)p(x_2|x_3,\\ldots,x_n,y)\\ldots p(x_n|y)p(y)\n",
    "\\end{align}\n",
    "\n",
    "Then the naive part assumes that the elements in the feature vector are independent of each other\n",
    "\n",
    "\\begin{align}\n",
    "p(y|X)&=p(x_1|x_2,\\ldots,x_n,y)p(x_2|x_3,\\ldots,x_n,y)\\ldots p(x_n|y)p(y)\\\\\n",
    "&\\approx p(x_1|y)p(x_2|y)\\ldots p(x_n|y)p(y)\n",
    "\\end{align}\n",
    "\n",
    "The quantities $p(x_1|y)$, $p(x_2|y)$, $\\ldots$, $p(x_n|y)$, $p(y)$ can be predicted from the data. A simple 1D Gaussian pdf can be estimated for each element of the feature vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
