{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Support Vector Machines (SVM) classifier </h1>\n",
    "\n",
    "Given the data set $\\{x_i,y_i\\}$ where $x_i\\in R^n$ are the features and $y_i\\in\\{-1,1\\}$ are the two possible classes. For simplicity let us split the data $x=\\{u_i,v_i\\}$ as $\\{u_1,u_2,\\ldots,u_M\\}$ having class $y=-1$(negative $<0$) and $\\{v_1,v_2,\\ldots,v_N\\}$ having class $y=1$(positive $>0$). The goal of svm is to classify a new input feature $x$ as 0 or 1. Assume that the predictor or classifier function is given as $y=f(x)$. Then this classifier has to classify the given training data i.e.\n",
    "\n",
    "\\begin{align}\n",
    "f(v_i)>0,& \\quad i=1,2,\\ldots,N \\\\\n",
    "f(u_j)<0,& \\quad i=1,2,\\ldots,M \n",
    "\\end{align}\n",
    "\n",
    "$f(x)$ can be any linear or nonlinear function. When $f(x)$ is of poynomial type, the resulting classifier becomes easy(computationally easy) to find.\n",
    "\n",
    "<h3>Linear SVM</h3>\n",
    "\n",
    "Assume that $f(x)=ax+b$, then the linear SVM problem is given as\n",
    "\n",
    "\\begin{align}\n",
    "av_i+b>0,& \\quad i=1,2,\\ldots,N \\\\\n",
    "au_j+b<0,& \\quad i=1,2,\\ldots,M \n",
    "\\end{align}\n",
    "\n",
    "the problem of finding the classifier is then reduced to finding the optimal a and b. These equations are homogeneous in a and b, i.e. if u find a and b then Ca and Cb are also solutions for any C. So these are modified in a way such that the homogenuity is removed\n",
    "\n",
    "\\begin{align}\n",
    "av_i+b\\ge 1,& \\quad i=1,2,\\ldots,N \\\\\n",
    "au_j+b\\le -1,& \\quad i=1,2,\\ldots,M \n",
    "\\end{align}\n",
    "\n",
    "As there are multiple solutions possible we can pose an optimization problem to find the best a and b as\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{a,b} \\quad \\|a\\|_2 \\\\\n",
    "av_i+b\\ge 1,& \\quad i=1,2,\\ldots,N \\\\\n",
    "au_j+b\\le -1,& \\quad i=1,2,\\ldots,M \n",
    "\\end{align}\n",
    "\n",
    "This is a convex optimization problem and can be solved very efficiently. The linear SVM can be made robust i.e. it is not biased towards any class.\n",
    "\n",
    "<h3> Robust Linear Classifier </h3>\n",
    "\n",
    "Instead of taking 1 in the inequality we can make it flexible by replacing 1 by a variable t and then maximize t such that it is as far away from 0 as possible. Further a is normalized as if a and b satisfy the constraints then Ca and Cb will also satify the constraints for any positive value for C.\n",
    "\n",
    "\\begin{align}\n",
    "& \\max_{t,a,b} \\quad t \\\\\n",
    "& av_i+b\\ge t, \\quad i=1,2,\\ldots,N \\\\\n",
    "& au_j+b\\le -t, \\quad i=1,2,\\ldots,M \\\\\n",
    "& \\|a\\|_2\\le 1\n",
    "\\end{align}\n",
    "\n",
    "<h3> Soft Constraint Robust Linear Classifier </h3>\n",
    "\n",
    "What if the data is not exactly separable by a line. In this case we can relax the constraints such that it can mis-classify few samples but can achieve a better overall classification.\n",
    "\n",
    "\\begin{align}\n",
    "& \\min_{p,q,a,b} \\quad 1^Tp+1^Tq \\\\\n",
    "& av_i+b\\ge 1-p_i, \\quad i=1,2,\\ldots,N \\\\\n",
    "& au_j+b\\le -(1-q_j), \\quad j=1,2,\\ldots,M \\\\\n",
    "\\end{align}\n",
    "\n",
    "<h3> Nonlinear Support Vector Machines </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Support Vector Machines (SVM) regression </h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
